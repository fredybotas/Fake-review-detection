{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import os \n",
    "cwd = os.getcwd()\n",
    "files = [a for a in glob.iglob(cwd + '**/**/*.txt', recursive=True)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>truthful</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>excellent staff and customer service, very cle...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>My stay at this hotel was one of the best I ha...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>We just got back from a trip to Chicago for my...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>I have to say that the Hard Rock Hotel in Chic...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>My husband and I recently stayed at the Hard R...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text sentiment truthful\n",
       "0  excellent staff and customer service, very cle...         1        0\n",
       "1  My stay at this hotel was one of the best I ha...         1        0\n",
       "2  We just got back from a trip to Chicago for my...         1        0\n",
       "3  I have to say that the Hard Rock Hotel in Chic...         1        0\n",
       "4  My husband and I recently stayed at the Hard R...         1        0"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "data = pd.DataFrame(columns=['text', 'sentiment', 'truthful'])\n",
    "\n",
    "for file in files:\n",
    "    f = open(file)\n",
    "    text = f.read()\n",
    "    real = 0\n",
    "    sentiment = 0\n",
    "    if 'truthful' in file:\n",
    "        real = 1\n",
    "    if 'positive_polarity' in file:\n",
    "        sentiment = 1\n",
    "    data = data.append({'text': text, 'sentiment': sentiment, 'truthful': real}, ignore_index=True)\n",
    "    \n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/fredybotas/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/fredybotas/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from nltk.corpus import stopwords \n",
    "from nltk.tokenize import word_tokenize\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import f_classif\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "data['urls_count'] = data['text'].apply(lambda x: len(re.findall('https?://(?:[-\\w.]|(?:%[\\da-fA-F]{2}))+', x)))\n",
    "data['words_count'] = data['text'].apply(lambda x: len(x.split()))\n",
    "data['sentences_count'] = data['text'].apply(lambda x: len(sent_tokenize(x)))\n",
    "data['words_per_sentence'] = data['words_count'] / data['sentences_count']\n",
    "\n",
    "data['text'] = data['text'].apply(lambda x: x.lower().strip())\n",
    "data['text'] = data['text'].apply(lambda x: re.sub(r\" +\", \" \", x))\n",
    "data['text'] = data['text'].apply(lambda x: re.sub(r\"https?:\\/\\/(www\\.)?[-a-zA-Z0-9@:%._\\+~#=]{2,256}\\.[a-z]{2,6}\\b([-a-zA-Z0-9@:%_\\+.~#?&//=]*)\", \"\", x))\n",
    "data['text'] = data['text'].apply(lambda x: re.sub(r\"/[-\\/\\\\^$*+?.()|[\\]{}]/g\", \"\", x))\n",
    "data['text'] = data['text'].apply(lambda x: re.sub(r\"[iex\\*][nsx\\*][ftx\\*][pjx\\*]\", \"\", x))\n",
    "data['text'] = data['text'].apply(lambda x: re.sub(r\"[0-9]+\", \"\", x))\n",
    "data['text'] = data['text'].apply(lambda x: re.sub(r\"#[a-zA-Z]+\", \"\", x))\n",
    "data['text'] = data['text'].apply(lambda x: re.sub(r\"['\\\";:,.?!\\/\\\\()\\[\\]+]\", \"\", x))\n",
    "data['text'] = data['text'].apply(lambda x: re.sub(r\"[-_]\", \" \", x))\n",
    "data['text'] = data['text'].apply(lambda x: re.sub(r\" +\", \" \", x))\n",
    "\n",
    "data = data[data['text'] != '']\n",
    "\n",
    "data = data[data['sentiment'] == 1]\n",
    "\n",
    "y = data['truthful']\n",
    "X = data.drop(columns='truthful')\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.python.keras.preprocessing import sequence\n",
    "from tensorflow.python.keras.preprocessing import text\n",
    "\n",
    "TOP_K = 30000\n",
    "\n",
    "MAX_SEQUENCE_LENGTH = 10000\n",
    "\n",
    "def sequence_vectorize(train_texts, val_texts):\n",
    "    tokenizer = text.Tokenizer(num_words=TOP_K)\n",
    "    tokenizer.fit_on_texts(train_texts)\n",
    "\n",
    "    x_train = tokenizer.texts_to_sequences(train_texts)\n",
    "    x_val = tokenizer.texts_to_sequences(val_texts)\n",
    "\n",
    "    max_length = len(max(x_train, key=len))\n",
    "    if max_length > MAX_SEQUENCE_LENGTH:\n",
    "        max_length = MAX_SEQUENCE_LENGTH\n",
    "\n",
    "    x_train = sequence.pad_sequences(x_train, maxlen=max_length)\n",
    "    x_val = sequence.pad_sequences(x_val, maxlen=max_length)\n",
    "    return x_train, x_val, tokenizer.word_index, max_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/fredybotas/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Model, Sequential\n",
    "from keras.layers import LSTM, Activation, Dense, Dropout, Input, Embedding\n",
    "from keras.layers.convolutional import Conv1D\n",
    "from keras.layers.convolutional import MaxPooling1D\n",
    "from keras.optimizers import RMSprop\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing import sequence\n",
    "from keras.utils import to_categorical\n",
    "from keras.callbacks import EarlyStopping\n",
    "\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "nltk.download('wordnet')\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "def remove_stop_words_and_lemmatize(data):\n",
    "    lemmatizer=WordNetLemmatizer()\n",
    "    res = []\n",
    "    for a in data:\n",
    "        input_str = word_tokenize(a)\n",
    "        temp_str = \"\"\n",
    "        for word in input_str:\n",
    "            if word in stop_words:\n",
    "                continue\n",
    "            temp_str += lemmatizer.lemmatize(word)\n",
    "            temp_str += \" \"\n",
    "        temp_str = temp_str[:-1]\n",
    "        res.append(temp_str)\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "x_train, x_val = X_train.text.values, X_test.text.values\n",
    "x_train = remove_stop_words_and_lemmatize(x_train)\n",
    "x_val = remove_stop_words_and_lemmatize(x_val)\n",
    "\n",
    "x_train, x_val, word_index, max_length = sequence_vectorize(x_train, x_val)\n",
    "num_features = min(len(word_index) + 1, TOP_K)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 640 samples, validate on 160 samples\n",
      "Epoch 1/5\n",
      "640/640 [==============================] - 10s 16ms/sample - loss: 0.6937 - accuracy: 0.5125 - val_loss: 0.6880 - val_accuracy: 0.6438\n",
      "Epoch 2/5\n",
      "640/640 [==============================] - 7s 10ms/sample - loss: 0.6744 - accuracy: 0.6172 - val_loss: 0.6427 - val_accuracy: 0.8000\n",
      "Epoch 3/5\n",
      "640/640 [==============================] - 7s 11ms/sample - loss: 0.4099 - accuracy: 0.8641 - val_loss: 0.3651 - val_accuracy: 0.8375\n",
      "Epoch 4/5\n",
      "640/640 [==============================] - 7s 11ms/sample - loss: 0.1418 - accuracy: 0.9594 - val_loss: 0.2790 - val_accuracy: 0.9062\n",
      "Epoch 5/5\n",
      "640/640 [==============================] - 7s 11ms/sample - loss: 0.0708 - accuracy: 0.9812 - val_loss: 0.2727 - val_accuracy: 0.8875\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x165fb3320>"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_vector_length = 256\n",
    "\n",
    "model = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Embedding(TOP_K, embedding_vector_length, input_length=max_length),\n",
    "    tf.keras.layers.Dropout(0.5),\n",
    "    tf.keras.layers.Conv1D(filters=32, kernel_size=3, padding='same', activation='relu'),\n",
    "    tf.keras.layers.MaxPooling1D(pool_size=2),\n",
    "    tf.keras.layers.Dropout(0.5),\n",
    "    tf.keras.layers.LSTM(32),\n",
    "    tf.keras.layers.Dropout(0.5),\n",
    "    tf.keras.layers.Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "model.fit(x_train, y_train, validation_data=(x_val, y_test), epochs=5, batch_size=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[74  8]\n",
      " [14 64]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.8621764705882352"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred = model.predict(x_val)\n",
    "y_pred = [1 if a > 0.5 else 0 for a in y_pred]\n",
    "\n",
    "print(confusion_matrix(list(y_test), list(y_pred)))\n",
    "\n",
    "f1_score(list(y_test), y_pred, average='weighted')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
